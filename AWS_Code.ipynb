{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "result = {}\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Sure that the \"saved_folder\" is already created in the same S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: enter initial document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial document details\n",
    "s3Name = #insert s3 bucket name\n",
    "docuName = #insert pdf document name\n",
    "saved_folder = #insert folder that output will go tm\n",
    "box_file_name = #insert box name\n",
    "csv_file_name = #insert end csv file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assuming API (Boto3) is already authenticated\n",
    "#### refer to https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html for Boto3 authenticatoin\n",
    "#### refer to https://docs.aws.amazon.com/textract/latest/dg/async.html for textract documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start Textract job\n",
    "def startJob(s3BucketName, objectName):\n",
    "    response = None\n",
    "    client = boto3.client(service_name='textract', region_name='us-west-2')\n",
    "    response = client.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': objectName\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return response[\"JobId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current status of the Textract job, checking in 5 second intervals\n",
    "def isJobComplete(jobId):\n",
    "    time.sleep(5)\n",
    "    client = boto3.client(service_name='textract', region_name='us-west-2')\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    "    status = response[\"JobStatus\"]\n",
    "    print(\"Job status: {}\".format(status))\n",
    "\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)\n",
    "        response = client.get_document_text_detection(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once job is complete get the results\n",
    "def getJobResults(jobId):\n",
    "    pages = []\n",
    "    time.sleep(5)\n",
    "    client = boto3.client(service_name='textract', region_name='us-west-2')\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    "    pages.append(response)\n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "\n",
    "    while(nextToken):\n",
    "        time.sleep(5)\n",
    "        response = client.get_document_text_detection(JobId=jobId, NextToken=nextToken)\n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start textract and check if job is complete\n",
    "def start_textract(s3BucketName, documentName):\n",
    "    jobId = startJob(s3BucketName, documentName)\n",
    "    print(\"Started job with id: {}\".format(jobId))\n",
    "    if(isJobComplete(jobId)):\n",
    "        response = getJobResults(jobId)\n",
    "        print ('uploading...')\n",
    "\n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the finished textract files back to the s3 bucket\n",
    "#this is needed for comprehend to run later\n",
    "def upload_text1(response):\n",
    "    text=[]\n",
    "    page=[]\n",
    "    file_name = \"Output1.txt\"\n",
    "    text_file = open(file_name, \"w\")\n",
    "    count = 0\n",
    "    for resultPage in response:\n",
    "        for item in resultPage[\"Blocks\"]:\n",
    "            if item[\"BlockType\"] == \"LINE\":\n",
    "                text.append(item[\"Text\"])\n",
    "                page.append(item['Page'])\n",
    "                if page[count] == page[count-1]:\n",
    "                    text_file.write(item[\"Text\"] + '\\n')\n",
    "                    count=count+1\n",
    "                else:\n",
    "                    text_file.close()\n",
    "                    s3_client = boto3.client('s3')\n",
    "                    response = s3_client.upload_file(file_name, s3Name, saved_folder + \"/{}\".format(file_name))\n",
    "                    file_name = \"Output\" + str(item['Page']) + \".txt\"\n",
    "                    text_file = open(file_name, \"w\")\n",
    "                    text_file.write(item[\"Text\"] +'\\n')\n",
    "                    count=count+1\n",
    "\n",
    "    #print(text)\n",
    "    text_file.close()\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function that calls other helper functions\n",
    "def convert_card_image():\n",
    "    result = start_textract(s3Name, docuName)\n",
    "    upload_text1(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN: function below to start textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run this call to start textract and upload the files to s3 bucket when finished\n",
    "convert_card_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: AWS Comprehend\n",
    "#### The below values are sample inputs give. The dARN, input, and outputs are specific to your model and file locations. \n",
    "#### For more information, refer: https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectEntities.html#comprehend-DetectEntities-request-EndpointArn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general comprehend\n",
    "general_dARN = \"arn:aws:iam::12318012:role/service-role/test-role\"\n",
    "general_s3_input = \"s3://test-path\"\n",
    "general_s3_output = \"s3://test-path\"\n",
    "\n",
    "#decision comprehend\n",
    "decision_dARN = \"arn:aws:iam::12318012:role/service-role/test-role\"\n",
    "decision_ARN = \"arn:aws:iam::12318012:role/service-role/test-role\"\n",
    "decision_s3_input = \"s3://test-path\"\n",
    "decision_s3_output = \"s3://test-path\"\n",
    "\n",
    "#custom comprehend\n",
    "custom_dARN = \"arn:aws:iam::12318012:role/service-role/test-role\"\n",
    "custom_ARN = \"arn:aws:iam::12318012:role/service-role/test-role\"\n",
    "custom_s3_input = \"s3://test-path\"\n",
    "custom_s3_output = \"s3://test-path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Comprehend Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### refer to https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html for general comprehend documentation\n",
    "#### refer to https://docs.aws.amazon.com/comprehend/latest/dg/auto-ml.html for custom comprehend documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts general comprehend analysis job\n",
    "def start_general_comprehend():\n",
    "    comprehend = boto3.client(service_name='comprehend', region_name='us-west-2')\n",
    "    response = comprehend.start_entities_detection_job(\n",
    "        DataAccessRoleArn = general_dARN,\n",
    "        LanguageCode = \"en\",\n",
    "        InputDataConfig= { \n",
    "            \"InputFormat\": \"ONE_DOC_PER_LINE\",\n",
    "            \"S3Uri\": general_s3_input  \n",
    "        },\n",
    "        OutputDataConfig= { \n",
    "            \"S3Uri\": general_s3_output\n",
    "        }\n",
    "    )\n",
    "    return response[\"JobId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts custom comprehend analysis job (decision entitiy only) \n",
    "def start_decision_comprehend():\n",
    "    comprehend = boto3.client(service_name='comprehend', region_name='us-west-2')\n",
    "    response = comprehend.start_entities_detection_job(\n",
    "        DataAccessRoleArn = decision_dARN,\n",
    "        EntityRecognizerArn= decision_ARN,\n",
    "        LanguageCode = \"en\",\n",
    "        InputDataConfig= { \n",
    "            \"InputFormat\": \"ONE_DOC_PER_LINE\",\n",
    "            \"S3Uri\": decision_s3_input \n",
    "        },\n",
    "        OutputDataConfig= { \n",
    "            \"S3Uri\": decision_s3_output\n",
    "        }\n",
    "    )\n",
    "    return response[\"JobId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts custom comprehend analysis job (other entities) \n",
    "def start_custom_comprehend():\n",
    "    comprehend = boto3.client(service_name='comprehend', region_name='us-west-2')\n",
    "    response = comprehend.start_entities_detection_job(\n",
    "        DataAccessRoleArn = custom_dARN,\n",
    "        EntityRecognizerArn= custom_ARN,\n",
    "        LanguageCode = \"en\",\n",
    "        InputDataConfig= { \n",
    "            \"InputFormat\": \"ONE_DOC_PER_LINE\",\n",
    "            \"S3Uri\": custom_s3_input\n",
    "        },\n",
    "        OutputDataConfig= { \n",
    "            \"S3Uri\": custom_s3_output\n",
    "        }\n",
    "    )\n",
    "    return response[\"JobId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current status of the comprehend job, checking in 20 second intervals\n",
    "def isJobComplete_comprehend(jobID, type_comprehend):\n",
    "    time.sleep(1)\n",
    "    client = boto3.client(service_name='comprehend', region_name='us-west-2')\n",
    "    response = client.describe_entities_detection_job(JobId=jobID)\n",
    "    \n",
    "    while(response['EntitiesDetectionJobProperties']['JobStatus'] == \"IN_PROGRESS\"):\n",
    "        response = client.describe_entities_detection_job(JobId=jobID)\n",
    "        print(\"working...\")\n",
    "        time.sleep(20)\n",
    "    \n",
    "    return response['EntitiesDetectionJobProperties']['JobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to start all comprehends together and print when each is done\n",
    "def start_all_comprehend():\n",
    "    general_id = start_general_comprehend()\n",
    "    decision_id = start_decision_comprehend()\n",
    "    comprehend_id = start_custom_comprehend()\n",
    "    general = isJobComplete_comprehend(general_id, \"general\")\n",
    "    decision = isJobComplete_comprehend(decision_id, \"decision\")\n",
    "    comprehend = isJobComplete_comprehend(comprehend_id, \"comprehend\")\n",
    "    print(general)\n",
    "    print(decision)\n",
    "    print(comprehend)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN: Start all comprehend functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calling function from above\n",
    "start_all_comprehend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: Comprehend Output File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_text_output = \"Custom.txt\"\n",
    "general_text_output = \"General.txt\"\n",
    "decision_text_output = \"Decision.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse through Comprehend Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse through general comprehend and identify banks or locations\n",
    "def initial_comprehend(file_name):\n",
    "    final_output = {}\n",
    "    file1 = open(\"General.txt\")\n",
    "    Lines = file1.readlines()\n",
    "    for line in Lines:\n",
    "        line_dict = json.loads(line)\n",
    "        if line_dict[\"File\"] == file_name:\n",
    "            final_output[line_dict[\"Line\"]] = {}\n",
    "            for entitiy in line_dict[\"Entities\"]:\n",
    "                final_output[line_dict[\"Line\"]][entitiy[\"Type\"]] = entitiy[\"Text\"]\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_normal(file_name):\n",
    "    custom_results = [\"No\", \"No\"]\n",
    "    basic = initial_comprehend(file_name)\n",
    "    for line in basic:\n",
    "        if custom_results[0] == \"No\":\n",
    "            if \"ORGANIZATION\" in basic[line]:\n",
    "                custom_results[0] = basic[line][\"ORGANIZATION\"]\n",
    "        if custom_results[1] == \"No\":\n",
    "            if \"LOCATION\" in basic[line]:\n",
    "                custom_results[1] = basic[line][\"LOCATION\"]\n",
    "    return custom_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse through custom comprehend and identify decision\n",
    "def if_decision (file_name, line_num):\n",
    "    file1 = open(\"Decision.txt\")\n",
    "    Lines = file1.readlines()\n",
    "    bank = \"NA\"\n",
    "    decision = \"\"\n",
    "    for line in Lines:\n",
    "        line_dict = json.loads(line)\n",
    "        if line_dict[\"File\"] == file_name:\n",
    "            if line_dict[\"Line\"] == line_num:\n",
    "                for text_type in line_dict[\"Entities\"]:\n",
    "                    if \"Type\" in text_type:\n",
    "                        if text_type[\"Type\"] == \"Decision\":\n",
    "                            decision = text_type[\"Text\"]\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse through custom comprehend\n",
    "def if_custom(file_name):\n",
    "    file1 = open(\"Custom.txt\")\n",
    "    Lines = file1.readlines()\n",
    "    bank = \"No\"\n",
    "    loc = \"No\"\n",
    "    for line in Lines:\n",
    "        line = str(line)\n",
    "        line_dict = json.loads(line)\n",
    "        if line_dict[\"File\"] == file_name:\n",
    "            for text_type in line_dict[\"Entities\"]:\n",
    "                if \"Type\" in text_type:\n",
    "                    if text_type[\"Type\"] == \"BANK\":\n",
    "                        bank = text_type[\"Text\"]\n",
    "                    elif text_type[\"Type\"] == \"LOCATION\":\n",
    "                        loc = text_type[\"Text\"]\n",
    "    return ([bank, loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare custom and general comprhend outputs to see if they are similar\n",
    "def compare(file_name):\n",
    "    custom_results = if_custom(file_name)\n",
    "    normal_results = if_normal(file_name)\n",
    "    if custom_results[0] == \"No\":\n",
    "        custom_results[0] = normal_results[0]\n",
    "    if custom_results[1] == \"No\":\n",
    "        custom_results[1] = normal_results[1]\n",
    "    return custom_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns abbrev if one exists within the line number (line_num) of the file (file_name)\n",
    "def abbrev_finder(file_name, line_num):\n",
    "    file1 = open(file_name, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    \n",
    "    line = Lines[line_num]\n",
    "    abbrev = re.search('[A-z]+.?-.?[0-9]+',line)\n",
    "    if abbrev:\n",
    "        return([abbrev.group(0), line])\n",
    "    else:\n",
    "        return [None,line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns paid if one exists within the line number (line_num) of the file (file_name)\n",
    "def paid_finder(file_name, num):\n",
    "    file1 = open(file_name, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    line = Lines[num]\n",
    "    paid = re.search('PAID+',line)\n",
    "    if paid:\n",
    "        return paid.group(0)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns bank if one exists within the line number (line_num) of the file (file_name)\n",
    "def get_bank(file_name):\n",
    "    file1 = open(file_name, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    for line in Lines:\n",
    "        bank2 = re.search('^Bank.*', line)\n",
    "        if bank2:\n",
    "            if \"Loan\" not in bank2.group(0):\n",
    "                return(bank2.group(0).split(\" - \")[0])\n",
    "        else:\n",
    "            bank2 = re.search('^bank.*',line)\n",
    "            if bank2:\n",
    "                if \"Loan\" not in bank2.group(0):\n",
    "                    return(bank2.group(0).split(\" - \")[0])\n",
    "        bank = re.search('.+?(?=Bank)',line)\n",
    "        if bank:\n",
    "            return(bank.group(0) + \"Bank\")\n",
    "        else:\n",
    "            bank = re.search('.+?(?=bank)',line)\n",
    "            if bank:\n",
    "                return(bank.group(0) + \"Bank\")\n",
    "            else:\n",
    "                continue\n",
    "    return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifies if county is in the line\n",
    "def county_in(file_name):\n",
    "    file1 = open(file_name)\n",
    "    Lines = file1.readlines()\n",
    "    for line in Lines:\n",
    "        if 'County' in line or 'county' in line:\n",
    "            return [1,line]\n",
    "            \n",
    "    return [0,\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Regex + Comprehend Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine regex + comprehend outputs to get all identified entities \n",
    "def rest_finder(file):\n",
    "    file2 = open('output18.txt','r')\n",
    "    lines = file2.readlines()\n",
    "    for line in lines:\n",
    "        line_dict = json.loads(line)\n",
    "        line_int = int(line_dict[\"Line\"])\n",
    "        line_number = \"line: \" + str(line_dict[\"Line\"])\n",
    "        result[line_number] = {}\n",
    "        for entity in line_dict[\"Entities\"]:\n",
    "            result[line_number][entity[\"Type\"]] = entity[\"Text\"]\n",
    "            result[line_number][\"Abbrev\"] = abbrev_finder(line_int)\n",
    "            result[line_number][\"Paid\"] = paid_finder(line_int)[0]\n",
    "            result[line_number][\"Line\"] = paid_finder(line_int)[1]\n",
    "            result[line_number][\"Page Number\"] = paid_finder(line_int)[2]\n",
    "    #return (result)\n",
    "    decision()\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile outputs into one singular csv file\n",
    "def get_csv_test():\n",
    "    wd = os.getcwd()\n",
    "    files = [i for i in os.listdir(wd) if i.endswith(\"txt\")]\n",
    "    with open (csv_file_name, 'w', newline = '') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Bank Name\", \"Location\", \"Date\", \"Abbrev\", \"Decision\", \"Amount\", \"Paid\", \"Box Number\", \"Page Number\", \"Line\", \"Bank_Confidence\", \"Loc_Confidence\", \"Line\", \"County\", \"County Text\"])\n",
    "        counter = 1\n",
    "        for file_name in files:\n",
    "            if file_name == \"Decision.txt\" or file_name == \"BankLocCode.txt\":\n",
    "                continue\n",
    "            print(\"Working on \" + str(counter))\n",
    "            counter += 1\n",
    "            both = compare(file_name)\n",
    "            custom = if_custom(file_name)\n",
    "            normal = if_normal(file_name)\n",
    "            if custom[0] == normal[0]:\n",
    "                bank_c = 1\n",
    "            else:\n",
    "                bank_c = 0\n",
    "            if custom[1] == normal[1]:\n",
    "                loc_c = 1\n",
    "            else:\n",
    "                loc_c = 0\n",
    "            general = initial_comprehend(file_name)\n",
    "            for line in general:\n",
    "                date = \"\"\n",
    "                abbrev = \"\"\n",
    "                decision = \"\"\n",
    "                amount = \"\"\n",
    "                paid = \"\"\n",
    "                box_number = box_file_name\n",
    "                page_number = re.search('[0-9]+',file_name).group(0)\n",
    "                line_str = \"\"\n",
    "                line_text = \"\"\n",
    "                if 'DATE' in general[line] or \"QUANTITY\" in general[line]:\n",
    "                    if 'DATE' in general[line]:\n",
    "                        date = general[line]['DATE']\n",
    "                    if 'QUANTITY' in general[line]:\n",
    "                        amount = general[line]['QUANTITY']\n",
    "                    print(file_name + \" , \" + str(line))\n",
    "                    abbrev_total = abbrev_finder(file_name, int(line))\n",
    "                    abbrev = abbrev_total[0]\n",
    "                    paid = paid_finder(file_name, int(line))\n",
    "                    decision = if_decision(file_name, int(line))\n",
    "                    line_str = str(int(line))\n",
    "                    line_text = [abbrev_total[1]][0]\n",
    "                    county = county_in(file_name)\n",
    "                    writer.writerow([both[0], both[1], date, abbrev, decision, amount, paid, box_number, page_number, line_str, str(bank_c), str(loc_c), line_text, county[0], county[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN: function to download csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calling above function\n",
    "get_csv_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
